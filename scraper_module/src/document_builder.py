"""Build review documents from scraper results."""

import os
import textwrap
import time
from typing import List, Dict, Any, Optional


def _slugify(text: str) -> str:
    """Create safe filename from text."""
    text = (text or "").strip().lower()
    for ch in [" ", "/", "\\", ":", ";", ",", "'"]:
        text = text.replace(ch, "-")
    return "".join(c for c in text if c.isalnum() or c in "-_") or "event"


def build_review_markdown(
    people: List[Dict[str, Any]],
    category: str,
    event_label: Optional[str] = None,
    output_dir: str = "scraper_module/output",
) -> str:
    """
    Create a Markdown review document for a single category.
    
    Args:
        people: List of people data
        category: Category name (e.g., 'speakers')
        event_label: Event name
        output_dir: Output directory
    
    Returns:
        Absolute path to generated file
    """
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    slug = _slugify(event_label or "")
    filename = f"{slug or 'event'}_{category}_{timestamp}.md"
    path = os.path.join(output_dir, filename)
    
    title = f"# {event_label or 'Event'} — {category.capitalize()} Review\n\n"
    
    intro = textwrap.dedent("""
        This document was generated by the AI scraper agent.
        Please carefully review each entry before using for outreach.
        
        ---
        
    """)
    
    lines = [title, intro]
    
    if not people:
        lines.append("_No candidates were extracted for this category._\n")
    else:
        for idx, person in enumerate(people, start=1):
            lines.append(f"## {idx}. {person.get('name') or 'Unknown'}\n")
            lines.append(f"- **Title**: {person.get('title') or 'N/A'}")
            lines.append(f"- **Company**: {person.get('company') or 'N/A'}")
            lines.append(f"- **Bio**: {person.get('bio') or 'N/A'}")
            lines.append(f"- **Email**: {person.get('email') or 'N/A'}")
            lines.append(f"- **LinkedIn**: {person.get('linkedin_url') or 'N/A'}")
            lines.append(f"- **Other URL**: {person.get('other_contact_url') or 'N/A'}")
            lines.append(f"- **Photo**: {person.get('photo_url') or 'N/A'}\n")
    
    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    
    return os.path.abspath(path)


def build_multi_category_review_markdown(
    results: Dict[str, List[Dict[str, Any]]],
    event_label: Optional[str] = None,
    event_type: Optional[str] = None,
    output_dir: str = "scraper_module/output",
) -> str:
    """
    Create comprehensive review document for multiple categories.
    
    Args:
        results: Dict mapping category -> list of people
        event_label: Event name
        event_type: Event type
        output_dir: Output directory
    
    Returns:
        Absolute path to generated file
    """
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    slug = _slugify(event_label or "")
    filename = f"{slug or 'event'}_review_{timestamp}.md"
    path = os.path.join(output_dir, filename)
    
    title = f"# {event_label or 'Event'}"
    if event_type:
        title += f" ({event_type})"
    title += " — Complete Review\n\n"
    
    intro = textwrap.dedent("""
        This document was generated by the AI scraper agent.
        Please carefully review each entry before using for outreach.
        
        The agent automatically classified the event and searched for relevant roles.
        
        ---
        
    """)
    
    lines = [title, intro]
    
    # Summary
    total = sum(len(p) for p in results.values() if isinstance(p, list))
    lines.append(f"## Summary\n")
    lines.append(f"- **Total people found**: {total}")
    lines.append(f"- **Categories**: {', '.join(results.keys())}\n")
    lines.append("---\n")
    
    # Each category
    for category, people in results.items():
        if not isinstance(people, list) or not people:
            continue
        
        lines.append(f"## {category.capitalize()} ({len(people)} found)\n")
        
        for idx, person in enumerate(people, start=1):
            lines.append(f"### {idx}. {person.get('name') or 'Unknown'}\n")
            lines.append(f"- **Title**: {person.get('title') or 'N/A'}")
            lines.append(f"- **Company**: {person.get('company') or 'N/A'}")
            lines.append(f"- **Bio**: {person.get('bio') or 'N/A'}")
            lines.append(f"- **Email**: {person.get('email') or 'N/A'}")
            lines.append(f"- **LinkedIn**: {person.get('linkedin_url') or 'N/A'}")
            lines.append(f"- **Other URL**: {person.get('other_contact_url') or 'N/A'}")
            lines.append(f"- **Photo**: {person.get('photo_url') or 'N/A'}\n")
        
        lines.append("\n---\n")
    
    if total == 0:
        lines.append("_No people were extracted._\n")
    
    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    
    return os.path.abspath(path)